= Analysis
include::_attributes.adoc[]

When performing upgrades of services there is a need to test the new version
that is being deployed to ensure functionality is not being negatively impacted. The
link:https://argo-rollouts.readthedocs.io/en/stable/features/analysis[Analysis,window='_blank'] feature
enables Rollouts to collect data and metrics from a variety of providers to validate the
new version of the application.

In addition to collecting data, an Analysis can include a Job to drive more advanced use cases. For example
a Job could be used to run load against an application in order to generate the metrics needed to
validate the revision.

To deploy an Analysis, first an AnalysisTemplate is created that defines the analysis that is required
and then is associated with one or more rollouts. The association with the rollout is dependent one
the rollout strategy used. In this module we will look at it from the perspective of the Blue-Green
strategy we have deployed however in the next module we delve into it for the Canary strategy as well.

In the blue-green strategy, an Analysis can be added as either pre-promotion or post-promotion. Pre-promotion
is used before the new version is deployed and is useful for validating the deployment prior to cutting over to it
for live traffic. Post-promotion is executed after the cut-over and can validate that the deployment is working
with live traffic.

[#analysis-deployment]
== Analysis Deployment

In this section we will deploy the same Blue-Green rollout we did previously but with
a pre-promotion analysis included along with the corresponding analysis template. Prior to starting,
confirm you are still at the correct path.

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
cd ~/argo-rollouts-workshop/documentation/modules/ROOT/examples/
----

Next, let's explore the manifests that we will be deploying in the `./bluegreen-analysis/base` folder:

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
ls ./bluegreen-analysis/base
----

Here you will see the same files that we used previously, however there is a new file `analysistemplate.yaml`. Examining
the file we see it appears as follows:

.link:https://github.com/OpenShiftDemos/argo-rollouts-workshop/blob/main/documentation/modules/ROOT/examples/bluegreen-analysis/base/analysistemplate.yaml[./bluegreen-analysis/base/analysistemplate.yaml,window='_blank']
[source,yaml,subs="+macros,attributes+"]
----
include::ROOT:example$bluegreen-analysis/base/analysistemplate.yaml[]
----

This AnalysisTemplate is broken up into two broad sections as follows:

* `.spec.args`. This is where you can specify arguments which are passed by the rollout when using the template. These arguments
enable the template to be reusable across many different rollouts.
* `.spec.metrics`. These are the metric providers that will be used to collect data for the analysis as well as any jobs
that need to be executed.

In the metrics provider section we can see we have two providers defined, one that uses the web metric provider to pull
metrics from Thanos, an aggregator for Prometheus data, and a job that runs link:https://github.com/JoeDog/siege[Apache Siege,window='_blank']
to drive some load on the application.

[NOTE]
We are using the web metric provider here since Argo Rollouts does not currently support authentication with the Prometheus
provider. This is one of the gaps that Red Hat is planning on closing prior to supporting Rollouts in OpenShift GitOps
as a Generally Available (GA) product.

The `count` and `interval` fields in the `success-rate` metric powered by the web provider indicate that the metric will be checked four times
with a thirty second interval between each check. The `failureLimit` determines how many failures are permitted for the rollout to be considered
a success, here we set a failure limit of 0.

Finally, remember we are running this Analysis in the pre-promotion phase of the blue-green strategy so the application will not
be receiving load from users, therefore generating load with Apache Siege will generate the metrics we need to determine whether
the promotion should proceed.

In the arguments we are taking two key arguments, `query` and `route-url`, which will be passed from the
Rollout. Notice that the `query` argument is being used at the end of the URL of the web provider as a
query parameter, this is the Prometheus query that we want to execute against the OpenShift monitoring stack.

[subs="quotes"]
----
provider:
  web:
    url: https://thanos-querier.openshift-monitoring:9091/api/v1/query?query={{ *args.query* }}
----

The `route-url` parameter is being used in the second provider, the job, where we will be creating load. This
parameter is used to specify the OpenShift route URL where we want to drive the load, i.e the URL that
siege will be hitting when it generates load.

Finally a third parameter, `api-token`, is provided by a secret. This secret provides a token needed to access the OpenShift monitoring
stack, it was created by the GitOps process which provisioned this workshop.

Next let's look at the rollout and see how the AnalysisTemplate is wired into the rollout.

.link:https://github.com/OpenShiftDemos/argo-rollouts-workshop/blob/main/documentation/modules/ROOT/examples/bluegreen-analysis/base/rollout.yaml[./bluegreen-analysis/base/rollout.yaml,window='_blank']
[source,yaml,subs="+macros,attributes+"]
----
include::ROOT:example$bluegreen-analysis/base/rollout.yaml[]
----

Notice that in `.spec.strategy.blueGreen` we have now defined the `prePromotionAnalysis` field. In this field
we define the template we want to use as well as the arguments that the rollout needs to provide the
analysis template.

[Note]
The query parameter is URL encoded since we are using the web provider rather then the prometheus
provider hence the odd encoding.

Next to deploy this new version of the blue-green rollout, execute the following command:

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
kustomize build ./bluegreen-analysis/base | sed "s/%SUB_DOMAIN%/$SUB_DOMAIN/" | sed "s/%USER%/%USERNUM%/" | oc apply -n user%USERNUM%-prod -f -
----

Check that the rollout has deployed successfully in the Argo Rollouts dashboard:

image::argo-rollout-dashboard-bg-analysis-initial.png[]

Next confirm that the application is displaying blue squares by opening the active route URL in your browser:

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
oc get route -n user%USERNUM%-prod active -o jsonpath='{"https://"}{.spec.host}{"\n"}'
----

image::rollouts-demo-app-blue.png[]

[#analysis-promotion]
== Promotion with Analysis

In this section we will promote new images using the Analysis to test the new version of the
application. In the first part we will perform a promotion where the analysis succeeds and
the promotion occurs. In the second part we will have the application trigger errors causing
the analysis to fail and the promotion to be aborted.

Note that auto-promotion is enabled.

=== Analysis Passes

With the updated blue-green rollout deployed, let's run through a promotion to a new image where
the analysis succeeds and observe the behavior. In the OpenShift console, go to the Pipelines
and do a promotion to a green image. As a reminder this is located here in the console:

image::console-pipelines-overview.png[]

Wait for the pipelines to complete and then go to the Argo Rollouts dashboard where
you may see the following if the Analysis is still running where the Analysis button
is shown in grey (highlighted in the image with a red outline):

image::argo-rollouts-analysis-in-progress-overview.png[]

When an a promotion of a rollout with an analysis is done the controller will
generate an analysis run for each template execution. Since we only have one template
being used in pre-promotion we will only see one analysis run being created however
it is possible for a promotion to have multiple analysis runs dependent on the
rollout configuration, i.e. in blue-green defining both pre and post promotion analysis.

Clicking on the analysis button will expand the view to show the in-progress analysis
that is running:

image::argo-rollouts-analysis-in-progress-details.png[]

Note there are two green boxes with graph icons that are being shown in separate
rows. Each row represents a provider and if you recall we had two providers, the job running
Siege and the metric provider. Hovering the mouse over the buttons will display
additional information:

image::argo-rollouts-analysis-in-progress-hover.png[]

Once the analysis is complete the Analysis button will go green to show that it successfully
completed:

image::argo-rollouts-analysis-completed.png[]

Notice that one row has four graph icons, this represents the four measurements that were taken
during the execution of the AnalysisRun. Hovering the mouse over any of these graph icons will
show you the measurement result.

[NOTE]
You may need to refresh the browser to see new metrics being added to the AnalysisRun depending
on connectivity during the workshop.

=== Analysis Fails

Now that we have experienced a successful promotion, let's examine the behavior when the
application fails. To start, open the preview version of the application in a separate
browser tab, remember we can retrieve the route for the preview service by using this command:

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
oc get route -n user%USERNUM%-prod preview -o jsonpath='{"https://"}{.spec.host}{"\n"}'
----

Slide the error bar as shown below all the way to the right. Notice how all of the flashing
squares have red boxes around them indicating they are returning error messages.

image::rollouts-demo-errors.png[]

[IMPORTANT]
In this section there is some timing involved, when we promote the new image the error
setting will be reset back to 0 for the preview service. After the pipeline completes,
immediately switch back to the tab with the preview service and push the error bar
back to 100%.

Now we will perform a promotion to a yellow image, go back to the OpenShift Pipeline and start it
with the yellow image:

image::console-pipeline-promote-yellow.png[]

As mentioned in the note above, once the pipeline is finished you must immediately switch back
to the preview service and set the error rate to 100%.

If you did everything correctly, you should see that the Analysis failed as indicated by the red
button:

image::argo-rollouts-analysis-failed.png[]

[NOTE]
If the timing did not work out for you, you can try again by promoting with a different color.
You can see the list of available colors .link:https://quay.io/repository/openshiftdemos/rollouts-demo?tab=tags[here, window='_blank']
where each color is simply a tag in our quay.io registry.

[#cleanup]
== Clean-up

Prior to moving on to the next module we need to perform some clean-up activities. First let's reset the Development environment back to blue:

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
oc apply -k ./deploy/base -n user%USERNUM%-dev
----

Next we will delete the Rollout in the `user%USERNUM%-prod` so we can start fresh as we explore the canary strategy next.

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
kustomize build ./bluegreen-analysis/base | sed "s/%SUB_DOMAIN%/$SUB_DOMAIN/" | sed "s/%USER%/%USERNUM%/" | oc delete -n user%USERNUM%-prod -f -
----
